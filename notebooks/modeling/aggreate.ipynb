{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setup\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "sys.path.insert(1, module_path + \"/utils\")\n",
    "\n",
    "## db setup\n",
    "# pip install sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass \n",
    "\n",
    "# pandas setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from modeling import lag_columns, extract_date_features\n",
    "from data_from_db import get_table_from_shelter\n",
    "from eda import print_correlation_matrix, cramers_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_table_from_shelter('shelter_climate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### focus on Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['location_city'] == 'Toronto']\n",
    "df['capacity_units'] = df['taken_units'] + df['free_units']\n",
    "df_model = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question/Use-Case: Give predictions per overnight_shelter type on a monthly base "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start with warming shelter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warming_shelter = df[df['overnight_service_type'] == 'Warming Centre']\n",
    "warming_shelter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_functions = {\n",
    "    'taken_units': 'sum',\n",
    "    'free_units': 'sum',\n",
    "    'capacity_units': 'sum',\n",
    "    'min_temperature': 'mean',\n",
    "    'total_precipitation': 'mean',\n",
    "    'mean_temperature': 'mean',\n",
    "    'max_temperature': 'mean',\n",
    "    'snow_on_ground': 'mean'\n",
    "}\n",
    "\n",
    "warming_daily = warming_shelter.groupby('date').agg(agg_functions).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warming_daily = extract_date_features(warming_daily, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warming_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate monthly averages\n",
    "monthly_averages = warming_daily.groupby([warming_daily['date'].dt.month]).agg({\n",
    "    'min_temperature': 'mean',\n",
    "    'total_precipitation': 'mean',\n",
    "    'mean_temperature': 'mean',\n",
    "    'max_temperature': 'mean',\n",
    "    'snow_on_ground': 'mean',\n",
    "    'taken_units': 'sum',\n",
    "    'free_units': 'sum',\n",
    "    'capacity_units': 'sum'\n",
    "}).reset_index()\n",
    "monthly_averages.rename(columns={'date': 'month'}, inplace=True)\n",
    "monthly_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_averages.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = monthly_averages.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix['capacity_units'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = monthly_averages['min_temperature']\n",
    "capacity_unit = monthly_averages['capacity_units']\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(temp, capacity_unit, alpha=0.5)\n",
    "plt.title('Scatter Plot of Min_Temp and Capacity Unit')\n",
    "plt.xlabel('temp')\n",
    "plt.ylabel('Capacity Unit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_averages = monthly_averages[['month', 'min_temperature','total_precipitation','snow_on_ground','capacity_units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = monthly_averages.corr()\n",
    "correlation_matrix['capacity_units'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X = monthly_averages[['min_temperature', 'total_precipitation', 'snow_on_ground']]\n",
    "y = monthly_averages['capacity_units']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"R^2 Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Plotting the diagonal line\n",
    "plt.title('Actual vs Predicted Capacity Units')\n",
    "plt.xlabel('Actual Capacity Units')\n",
    "plt.ylabel('Predicted Capacity Units')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X = monthly_averages[['month','min_temperature', 'total_precipitation', 'snow_on_ground']]\n",
    "y = monthly_averages['capacity_units']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"R^2 Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> too little data in the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no filtering by overnight_service_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_month = extract_date_features(df, 'date')\n",
    "by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = by_month.select_dtypes(include='number')\n",
    "correlation_matrix = numeric_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix['capacity_units'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **realised that it will always only be 12 rows when i aggregate..**\n",
    "\n",
    "#### instead: try utilizing extracted months \n",
    "- build a model that checks the month column -- make the time so small that it seems linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.drop(columns='location_city', inplace=True)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter for specific month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- extract dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = extract_date_features(df_model, 'date')\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = df_model.select_dtypes(include=['number'])\n",
    "numerical['date'] = df_model['date']\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_columns(df, date_column, columns_to_lag, lag_column_name):\n",
    "    # Convert the date column to datetime\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Group the DataFrame by date and calculate the mean for each specified column\n",
    "    daily_mean_temp_df = df.groupby(df[date_column].dt.date)[columns_to_lag].mean().reset_index()\n",
    "    \n",
    "    # Create a new DataFrame for lagged columns\n",
    "    lagged_columns_df = pd.DataFrame()\n",
    "    \n",
    "    # Create lagged columns for each specified column\n",
    "    for column in columns_to_lag:\n",
    "        # Calculate the lagged values\n",
    "        lagged_column_name = f\"{column}_{lag_column_name}\"\n",
    "        daily_mean_temp_df[lagged_column_name] = daily_mean_temp_df[column].shift(1)\n",
    "        # Fill NaN values with the first value of the column\n",
    "        daily_mean_temp_df[lagged_column_name].fillna(daily_mean_temp_df[column].iloc[0], inplace=True)\n",
    "        \n",
    "        # Add lagged column to lagged_columns_df\n",
    "        lagged_columns_df[lagged_column_name] = daily_mean_temp_df[lagged_column_name]\n",
    "    \n",
    "    # Add date column to lagged_columns_df\n",
    "    lagged_columns_df['date_lag'] = daily_mean_temp_df[date_column]\n",
    "    \n",
    "    return lagged_columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_lag = ['mean_temperature','snow_on_ground', 'capacity_units']\n",
    "lagged_data = add_lagged_columns(numerical, 'date', columns_to_lag, '_1')\n",
    "display(lagged_data.isna().sum().sum())\n",
    "lagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical.reset_index(drop=True, inplace=True)\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical['date'] = pd.to_datetime(numerical['date'])\n",
    "lagged_data['date_lag'] = pd.to_datetime(lagged_data['date_lag'])\n",
    "merged_df = pd.merge(numerical, lagged_data, left_on='date', right_on='date_lag', how='left')\n",
    "numerical = merged_df.drop(columns='date_lag').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = extract_date_features(numerical, 'date')\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = numerical[numerical['month'] == 2]\n",
    "numerical.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check correlation matrix numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_correlation_matrix(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = numerical.corr()\n",
    "correlation_matrix['capacity_units'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- remove columns with low correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = numerical[['capacity_units','capacity_units__1','snow_on_ground','snow_on_ground__1','mean_temperature','mean_temperature__1']]\n",
    "numerical.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check correlation categorical and numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df_model.select_dtypes(object)\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = categorical[df_model['month'] == 2]\n",
    "categorical['date'] = df_model['date']\n",
    "categorical.reset_index(drop=True, inplace=True)\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical['sector'] = categorical['sector'].replace(['Women', 'Men', 'Mixed Adult'], 'Adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cramér's V for each categorical variable against the target variable\n",
    "for col in categorical.columns[:-1]:  # Exclude the target variable\n",
    "    cramers_v_score = cramers_v(categorical[col], numerical['capacity_units'])\n",
    "    print(f\"Cramér's V for {col}: {cramers_v_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([numerical, categorical], axis=1)\n",
    "X['capacity_units__1'] = X['capacity_units__1'].round(1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['capacity_units']\n",
    "X = X.drop(columns='capacity_units')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate cat from num\n",
    "numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = X.select_dtypes(object).columns\n",
    "\n",
    "X_train_numerical = X_train[numerical_columns]\n",
    "X_train_categorical = X_train[categorical_columns]\n",
    "\n",
    "X_test_numerical = X_test[numerical_columns]\n",
    "X_test_categorical = X_test[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- normalize categorical - one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### one hot encode \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_train_categorical.columns = [str(col) for col in X_train_categorical.columns]\n",
    "X_test_categorical.columns = [str(col) for col in X_test_categorical.columns]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_data = encoder.fit_transform(X_train_categorical)\n",
    "\n",
    "X_train_cat_hot = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(X_train_categorical.columns))\n",
    "display(X_train_cat_hot.head())\n",
    "\n",
    "# encode test using train encoder\n",
    "encoded_test_data = encoder.transform(X_test_categorical)\n",
    "X_test_cat_hot = pd.DataFrame(encoded_test_data, columns=encoder.get_feature_names_out(X_train_categorical.columns))\n",
    "\n",
    "display(X_test_cat_hot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scale numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train_numerical.columns = [str(col) for col in X_train_numerical.columns]\n",
    "X_test_numerical.columns = [str(col) for col in X_train_numerical.columns]\n",
    "# Initialize MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform Min-Max Scaling on numerical data\n",
    "X_train_numerical_scaled = minmax_scaler.fit_transform(X_train_numerical)\n",
    "\n",
    "# Convert the scaled numerical data back to a DataFrame\n",
    "X_train_numerical_scaled_df = pd.DataFrame(X_train_numerical_scaled, columns=X_train_numerical.columns)\n",
    "\n",
    "# Combine the scaled numerical data with the original categorical data\n",
    "X_train_scaled = pd.concat([X_train_numerical_scaled_df, X_train_categorical], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
